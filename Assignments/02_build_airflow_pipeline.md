# ðŸ“˜ Student Assignment: Build Airflow DAG

# Assignment: Hourly Support Call Enrichment Pipeline (Airflow/MySQL/JSON/DuckDB)

## Business reason
Support teams need near-real-time visibility into call quality and context.  
This pipeline enriches raw support call logs with telephony metadata and an LLM-style call summary, then loads a clean analytical table for reporting, QA, and monitoring.

---

## Assignment goal
Build an **Apache Airflow DAG** that runs **every hour** and incrementally processes support call data:
- Reads new calls from MySQL
- Enriches them with telephony data from JSON files (mock API)
- Writes the final enriched dataset into DuckDB

---

## What you must build

### Sources
#### MySQL â€“ Support Call Centre Database
Tables:
- `employees`
- `calls`

Students must:
- Generate **~50 employee records**
- Generate sample call records where **new calls appear over time**

#### Telephony Service (Mocked API)
- Instead of a real API, students **generate JSON files**
- These files simulate responses from a telephony provider.

Each JSON record must contain:
- `call_id`
- `duration_sec`
- `short_description` (pretend this was generated by an LLM)

Students must **read and parse JSON files in the DAG**.

---

## Output
- A **DuckDB** database with a table containing **enriched support calls**
- Example table name: `support_call_enriched`

---

## Data requirements

### MySQL
#### `employees`
- ~50 records
- Example fields:
  - `employee_id`
  - `full_name`
  - `team` / `role`
  - `hire_date`

#### `calls`
- Example fields:
  - `call_id` (primary key)
  - `employee_id` (foreign key)
  - `call_time`
  - `phone`
  - `direction`
  - `status`

The pipeline **must load only new calls** since the last successful DAG run.

---

### Telephony JSON (Mock API)
For each new `call_id`, a JSON file must exist with:
- `call_id`
- `duration_sec`
- `short_description`

The DAG must:
- Load JSON dynamically
- Validate required fields
- Handle missing or invalid JSON gracefully

---

## DAG requirements

### Schedule
- Runs **every hour**

### Airflow Connections
- Use **Airflow Connections** for MySQL access  
- Do **not** hardcode credentials

---

## DAG structure (recommended best-practice design)

### 1. `detect_new_calls`
- Query MySQL for calls with: call_time > last_loaded_call_time
- Store and read the watermark using:
  - Airflow Variables **or**
  - A metadata table in DuckDB
- Output list of new `call_id`s (via XCom)

---

### 2. `load_telephony_details`
- For new call IDs:
  - Read corresponding JSON files
  - Parse and validate schema
  - Output parsed telephony records (XCom or staging file)

---

### 3. `transform_and_load_duckdb`
- Join:
  - `calls`
  - `employees`
  - telephony JSON data
  - Insert or upsert into DuckDB
  - Update watermark **only after successful load**

---

## Important implementation notes
- Tasks must be **idempotent**
- Re-running the DAG must **not duplicate data**
- Use primary key (`call_id`) or merge logic in DuckDB
- Prefer hooks/operators instead of raw connections

---

## Evaluation criteria (max 10 points)

| Category | What is checked | Points |
|-------|----------------|--------|
| **Theory & code understanding** | Student can explain DAG purpose, task responsibilities, scheduling, XComs, incremental logic, idempotency, and retries | **3.0** |
| DAG scheduling & structure | Hourly schedule, clear task separation, correct dependencies | 1.5 |
| Airflow Connections usage | MySQL accessed via Airflow Connection (no hardcoded credentials) | 1.0 |
| Incremental logic | Correct new-call detection and watermark handling | 1.5 |
| JSON mock API parsing | JSON loading, schema validation, basic error handling | 1.5 |
| Join correctness | Proper joins, no duplicated or dropped rows | 1.0 |
| DuckDB load | Correct inserts/upserts into DuckDB | 0.5 |
| **Total** |  | **10.0** |

---

## Additional points (up to +2.5)

| Extra task | Points |
|-----------|--------|
| Data quality checks (duration â‰¥ 0, employee exists, unique call_id) | +1.0 |
| Observability (logging row counts, rejected JSONs) | +0.5 |
| Backfill-friendly DAG (catchup, start_date, deterministic runs) | +0.5 |
| Retry & alert strategy (retries, retry_delay, callbacks) | +0.5 |

**Maximum possible score: 12.5 points**

---

## Deliverables
- Airflow DAG code
- SQL scripts for MySQL table creation and data generation
- Sample telephony JSON files
- DuckDB output file
- This completed assignment README
---

## Theory questions (Airflow)

1. What is a **DAG** in Apache Airflow and what problem does it solve?

2. What is a **task** in Airflow and how is it different from a DAG?

3. What is an **Operator** in Airflow?  
   Give one example of an operator that could be used in this assignment.

4. What is the difference between **Operators** and **Hooks** in Airflow?

5. What is **XCom** and when should it be used?  
   Give an example of how XCom is used in this DAG.

6. What is an **Airflow Connection** and why should credentials not be hardcoded in DAGs?

7. What are **Airflow Variables** and when would you use them instead of XCom?

8. What is a **cron expression** and how is it used to schedule an Airflow DAG?  
   Give an example of a cron expression for running a DAG every hour.

9. What does the **catchup** parameter do in Airflow?  
   Why is `catchup=False` often used for hourly production pipelines?

10. What does **idempotency** mean in data pipelines and why is it important for Airflow DAGs?
